{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jack-xu8/ai-vs-human-music-classification/blob/main/Ai_Human_Music_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6fHeq2_NbZ3"
      },
      "source": [
        "# Processing the Datasets for Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smHB6QLpTEYE"
      },
      "source": [
        "The goal of this section is to align formats of the datasets. In the first block, I try to find the keys of the .npz files and check the shapes to see if the files have comparable formats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "V1_5OG14PGod",
        "outputId": "782e72f9-34ed-4c28-8446-359437178983"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-34d91575ceee>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Function to inspect keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minspect_npz_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf_keras.src.optimizers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"keras.src.optimizers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m   \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# DO NOT EDIT. Generated by api_gen.sh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDTypePolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFloatDTypePolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/api/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/api/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mobject_registration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialization_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_registration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_registered_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_registration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_keras_serializable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialization_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialization_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlegacy_h5_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaving_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfile_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaving_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mobject_registration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/saving_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialization\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlegacy_serialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_build_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_mapping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_saveable_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaving_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbase_trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraceback_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompileLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompileMetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_adapter_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraceback_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribution_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_data_adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpy_dataset_adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_data_adapter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrayDataAdapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_adapter_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataAdapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/array_slicing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpandas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# numpy compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     from pandas.compat import (\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mis_numpy_dev\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_is_numpy_dev\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompressors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_numpy_dev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m from pandas.compat.pyarrow import (\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mpa_version_under10p1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mpa_version_under11p0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/compat/pyarrow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0m_palv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mmodule_from_spec\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_init_module_attrs\u001b[0;34m(spec, module, override)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# Function to inspect keys\n",
        "def inspect_npz_keys(file_path):\n",
        "    data = np.load(file_path, allow_pickle=True, encoding='latin1')\n",
        "    print(f\"Keys in {file_path}: {data.files}\")\n",
        "    for key in data.files:\n",
        "        print(f\"Key: {key}, Type: {type(data[key])}, Shape: {getattr(data[key], 'shape', 'N/A')}\")\n",
        "\n",
        "inspect_npz_keys('/content/datasets/Jsb16thSeparated.npz')\n",
        "inspect_npz_keys('/content/datasets/js-fakes-16thSeparated.npz')\n",
        "# Test out what a pitches chorale's shape is\n",
        "jsf = np.load('/content/datasets/js-fakes-16thSeparated.npz', allow_pickle=True, encoding='latin1')\n",
        "print(\"Example structure of a 'pitches' chorale:\", type(jsf['pitches'][0]), jsf['pitches'][0].shape)\n",
        "# Test out what a train chorale's shape is\n",
        "jsb = np.load('/content/datasets/Jsb16thSeparated.npz', allow_pickle=True, encoding='latin1')\n",
        "print(\"Example structure of a 'train' chorale:\", type(jsb['train'][0]), jsb['train'][0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2Skb8EcTzgh"
      },
      "source": [
        "In this block, I try to combine the two datasets into different forms of data for my models. For the sequence based ones, I use sequences. For the 'simpler' models, I extract specific features to analyze."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mycWI1ObPbjF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "\n",
        "# Get singular arrays for each dataset\n",
        "jsb_combined = np.concatenate([jsb['train'], jsb['valid'], jsb['test']])\n",
        "jsf_pitches = jsf['pitches']\n",
        "\n",
        "# Assign labels\n",
        "y_jsb = ['Human'] * len(jsb_combined)\n",
        "y_jsf = ['AI'] * len(jsf_pitches)\n",
        "\n",
        "# Replace NaN with -1 in the JSB Chorales dataset for consistency with JS Fakes\n",
        "jsb_combined = [np.nan_to_num(chorale, nan=-1) for chorale in jsb_combined]\n",
        "\n",
        "# Combine data into sequences\n",
        "jsb_combined = list(jsb_combined)\n",
        "jsf_pitches = list(jsf_pitches)\n",
        "X_sequences = jsb_combined + jsf_pitches\n",
        "y_sequences = ['Human'] * len(jsb_combined) + ['AI'] * len(jsf_pitches)\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode 'Human' as 1, and 'AI' as 0\n",
        "y_encoded = label_encoder.fit_transform(y_sequences)\n",
        "\n",
        "# Split for sequence based models\n",
        "X_train_sequence, X_test_sequence, y_train_sequence, y_test_sequence = train_test_split(X_sequences, y_encoded, test_size=0.2, random_state=42)\n",
        "X_train_ragged = tf.ragged.constant(X_train_sequence)\n",
        "X_test_ragged = tf.ragged.constant(X_test_sequence)\n",
        "max_length = 640\n",
        "X_train_dense = X_train_ragged.to_tensor(default_value=0.0, shape=(None, max_length, 4))\n",
        "X_test_dense = X_test_ragged.to_tensor(default_value=0.0, shape=(None, max_length, 4))\n",
        "X_new_train_sequence, X_val_sequence, y_new_train_sequence, y_val_sequence = train_test_split(X_train_sequence, y_train_sequence, test_size=0.25, random_state=42)\n",
        "X_new_ragged = tf.ragged.constant(X_new_train_sequence)\n",
        "X_new_dense = X_new_ragged.to_tensor(default_value=0.0, shape=(None, max_length, 4))\n",
        "X_val_ragged = tf.ragged.constant(X_val_sequence)\n",
        "X_val_dense = X_val_ragged.to_tensor(default_value=0.0, shape=(None, max_length, 4))\n",
        "y_new_train_sequence = tf.expand_dims(y_new_train_sequence, axis=-1)\n",
        "y_val_sequence = tf.expand_dims(y_val_sequence, axis=-1)\n",
        "\n",
        "# Feature extraction for feature based models\n",
        "def extract_features(sequence):\n",
        "    features = {}\n",
        "\n",
        "    # Edge case handler (probably not necessary)\n",
        "    if sequence.size == 0:\n",
        "        return {key: 0 for key in [\n",
        "            'silent_voices', 'average_pitch', 'pitch_range',\n",
        "            'melodic_movement', 'parallel_motion',\n",
        "            'voice_crossings', 'note_density',\n",
        "            'rhythmic_variance', 'sustain_ratio', 'pitch_entropy'\n",
        "        ]}\n",
        "\n",
        "    # Silence\n",
        "    total_elements = sequence.size\n",
        "    silent_count = np.sum(sequence == -1)\n",
        "    features['silent_voices'] = silent_count / total_elements\n",
        "\n",
        "    # Pitch-related features\n",
        "    valid_pitches = sequence[sequence != -1]\n",
        "    if len(valid_pitches) > 0:\n",
        "        features['average_pitch'] = np.mean(valid_pitches)\n",
        "        features['pitch_range'] = np.max(valid_pitches) - np.min(valid_pitches)\n",
        "    else:\n",
        "        features['average_pitch'] = 0\n",
        "        features['pitch_range'] = 0\n",
        "\n",
        "    # Melodic movement\n",
        "    diffs = np.diff(sequence, axis=0)\n",
        "    melodic_movement = np.abs(diffs[sequence[1:] != -1])\n",
        "    features['melodic_movement'] = np.mean(melodic_movement) if len(melodic_movement) > 0 else 0\n",
        "\n",
        "    # Parallel motion\n",
        "    prev_intervals = sequence[:-1, :, None] - sequence[:-1, None, :]\n",
        "    curr_intervals = sequence[1:, :, None] - sequence[1:, None, :]\n",
        "    valid_intervals = (sequence[:-1, :, None] != -1) & (sequence[:-1, None, :] != -1)\n",
        "    parallel_motion = np.sum((prev_intervals == curr_intervals) & valid_intervals)\n",
        "    total_valid_intervals = np.sum(valid_intervals)\n",
        "    features['parallel_motion'] = parallel_motion / total_valid_intervals if total_valid_intervals > 0 else 0\n",
        "\n",
        "    # Voice crossings\n",
        "    valid_mask = (sequence != -1)\n",
        "    crossings = np.sum((sequence[:, :, None] < sequence[:, None, :]) & valid_mask[:, :, None] & valid_mask[:, None, :])\n",
        "    features['voice_crossings'] = crossings\n",
        "\n",
        "    # Rhythmic features\n",
        "    active_notes = np.sum(sequence != -1, axis=1)  # Active notes per timestep\n",
        "    features['note_density'] = np.mean(active_notes)\n",
        "    features['rhythmic_variance'] = np.std(active_notes)\n",
        "\n",
        "    # Sustain ratios (vectorized)\n",
        "    sustain_count = np.sum(sequence[1:] == sequence[:-1])\n",
        "    features['sustain_ratio'] = sustain_count / total_elements\n",
        "\n",
        "    # Pitch entropy\n",
        "    if len(valid_pitches) > 0:\n",
        "        unique_pitches, pitch_counts = np.unique(valid_pitches, return_counts=True)\n",
        "        pitch_probs = pitch_counts / len(valid_pitches)\n",
        "        features['pitch_entropy'] = -np.sum(pitch_probs * np.log2(pitch_probs))\n",
        "    else:\n",
        "        features['pitch_entropy'] = 0\n",
        "\n",
        "    return features\n",
        "\n",
        "X_features = [extract_features(seq) for seq in X_sequences]\n",
        "X_features_df = pd.DataFrame(X_features)\n",
        "X_train_features, X_test_features, y_train_features, y_test_features = train_test_split(X_features_df, y_encoded, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRmnNc0AeOYt"
      },
      "source": [
        "# Implementing the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUhQHCQJeXrt"
      },
      "source": [
        "In this block, I implement a LSTM, RNN, and a GRU, or Gated Recurrent Unit. I also use a pretrained transformer just to see how well it can be adapted to this task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtgK8KnmfMLV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Input, SimpleRNN, Dense, LSTM, GRU\n",
        "from tensorflow.keras.models import Model\n",
        "from transformers import TFAutoModel\n",
        "from tensorflow.keras.metrics import AUC, F1Score\n",
        "\n",
        "# Models\n",
        "model_LSTM = Sequential([\n",
        "    Input(shape=(max_length, 4)),\n",
        "    LSTM(128, return_sequences=False),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_RNN = Sequential([\n",
        "    Input(shape=(max_length, 4)),\n",
        "    SimpleRNN(128, return_sequences=False),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_GRU = Sequential([\n",
        "    Input(shape=(max_length, 4)),\n",
        "    GRU(128, return_sequences=False),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "# Compile the models\n",
        "model_LSTM.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', F1Score(name='f1-score'), AUC(name='roc_auc')])\n",
        "model_RNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', F1Score(name='f1-score'), AUC(name='roc_auc')])\n",
        "model_GRU.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', F1Score(name='f1-score'), AUC(name='roc_auc')])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7OH_h-Zhg15"
      },
      "source": [
        "In this block, I implement the feature based models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aal9zEVNhvGF"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize the models\n",
        "k_NN = KNeighborsClassifier(n_neighbors=5)\n",
        "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "svm = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "logistic_regression = LogisticRegression(random_state=42)\n",
        "naive_bayes = GaussianNB()\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=200, random_state=42)\n",
        "decision_tree = DecisionTreeClassifier(random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si3Op5Wtvllb"
      },
      "source": [
        "In this block I implement a CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbgYS0fdvTm9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input\n",
        "\n",
        "X_train_cnn = np.expand_dims(X_train_dense, axis=-1)\n",
        "X_new_cnn = np.expand_dims(X_new_dense, axis=-1)\n",
        "X_val_cnn = np.expand_dims(X_val_dense, axis=-1)\n",
        "X_test_cnn = np.expand_dims(X_test_dense, axis=-1)\n",
        "# CNN model\n",
        "model_CNN = Sequential([\n",
        "    Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2], 1)),\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 1)),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 1)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the CNN\n",
        "model_CNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', AUC(name='roc_auc'), F1Score(name='f1-score')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ1k9wXwjjUj"
      },
      "source": [
        "In this block I do the initial training of the sequence based models, and track the training time and results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCZYatMTkmUd"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# A function that trains a model and measures the training time\n",
        "def train_time(model, train_dataset, val_dataset, epochs=10):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        history = model.fit(\n",
        "            train_dataset,\n",
        "            validation_data=val_dataset,\n",
        "            epochs=epochs,\n",
        "            verbose=1\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error during model.fit: {e}\")\n",
        "        return None, None  # Return None if fit fails\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "    return history, training_time\n",
        "\n",
        "#Train the models\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_new_dense, y_new_train_sequence)).batch(32)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_dense, y_val_sequence)).batch(32)\n",
        "for batch_x, batch_y in train_dataset.take(1):\n",
        "    print(\"Input batch shape:\", batch_x.shape)\n",
        "    print(\"Target batch shape:\", batch_y.shape)\n",
        "print(\"Training LSTM...\")\n",
        "lstm_history, lstm_time = train_time(model_LSTM, train_dataset, val_dataset)\n",
        "print(\"Training GRU...\")\n",
        "gru_history, gru_time = train_time(model_GRU, train_dataset, val_dataset)\n",
        "print(\"Training RNN...\")\n",
        "rnn_history, rnn_time = train_time(model_RNN, train_dataset, val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osROO3-Surkd"
      },
      "source": [
        "In this block I do the initial training of the feature based models, and track the training time and results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMZh_4bTuq8b"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_features)\n",
        "X_test_scaled = scaler.transform(X_test_features)\n",
        "\n",
        "models = [\n",
        "    (\"k-NN\", k_NN),\n",
        "    (\"Random Forest\", random_forest),\n",
        "    (\"SVM\", svm),\n",
        "    (\"Logistic Regression\", logistic_regression),\n",
        "    (\"Naive Bayes\", naive_bayes),\n",
        "    (\"MLP\", mlp),\n",
        "    (\"Decision Tree\", decision_tree),\n",
        "]\n",
        "results = {}\n",
        "\n",
        "# Train each model\n",
        "for name, model in models:\n",
        "    print(f\"Training {name}...\")\n",
        "\n",
        "    # Measure training time\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train_scaled, y_train_features)  # Train the model\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
        "    accuracy = accuracy_score(y_test_features, y_pred)\n",
        "    f1 = f1_score(y_test_features, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test_features, y_pred_proba) if y_pred_proba is not None else None\n",
        "\n",
        "    # Print the results\n",
        "    results[name] = {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"F1-Score\": f1,\n",
        "        \"ROC-AUC\": roc_auc,\n",
        "        \"Training Time (s)\": training_time,\n",
        "    }\n",
        "    print(f\"Accuracy: {accuracy:.3f}\")\n",
        "    print(f\"F1-Score: {f1:.3f}\")\n",
        "    if roc_auc is not None:\n",
        "        print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "    else:\n",
        "        print(\"ROC-AUC: Not available\")\n",
        "    print(f\"Training Time: {training_time:.2f} seconds\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BqOygCyv4bo"
      },
      "source": [
        "In this block I do the initial training of the CNN, and track the training time and results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tj2LYRg9v8X0"
      },
      "outputs": [],
      "source": [
        "# Train the CNN\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_new_cnn, y_new_train_sequence)).batch(32)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_cnn, y_val_sequence)).batch(32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_cnn, y_test_sequence)).batch(32)\n",
        "print(\"Training CNN(sequences)...\")\n",
        "cnn_history, cnn_time = train_time(model_CNN, train_dataset, val_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOjW-xAgzR5L"
      },
      "source": [
        "# Initial Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E21SvX4bzWZP"
      },
      "source": [
        "This block is for creating a visual representation of the training results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGQ_SuLP3dyQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_feature_models(models, X_test_features, y_test_features):\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        # Predict probabilities\n",
        "        y_pred_probs = model.predict_proba(X_test_features)[:, 1] if hasattr(model, \"predict_proba\") else model.predict(X_test_features)\n",
        "        y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test_features, y_pred)\n",
        "        f1 = f1_score(y_test_features, y_pred)\n",
        "        roc_auc = roc_auc_score(y_test_features, y_pred_probs)\n",
        "\n",
        "        # Store results\n",
        "        results[name] = {\"Accuracy\": accuracy, \"F1-Score\": f1, \"ROC-AUC\": roc_auc}\n",
        "    return results\n",
        "\n",
        "def evaluate_sequence_models(models, X_test_sequence, y_test_sequence):\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        # Predict probabilities\n",
        "        y_pred_probs = model.predict(X_test_sequence)\n",
        "        y_pred = (y_pred_probs >= 0.5).astype(int)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test_sequence, y_pred)\n",
        "        f1 = f1_score(y_test_sequence, y_pred)\n",
        "        roc_auc = roc_auc_score(y_test_sequence, y_pred_probs)\n",
        "\n",
        "        # Store results\n",
        "        results[name] = {\"Accuracy\": accuracy, \"F1-Score\": f1, \"ROC-AUC\": roc_auc}\n",
        "    return results\n",
        "\n",
        "def evaluate_cnn_model(model, X_test_cnn, y_test_cnn):\n",
        "    # Predict probabilities\n",
        "    y_pred_probs = model.predict(X_test_cnn)\n",
        "    y_pred = (y_pred_probs >= 0.5).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test_cnn, y_pred)\n",
        "    f1 = f1_score(y_test_cnn, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test_cnn, y_pred_probs)\n",
        "    # Store results\n",
        "    results = {\"Accuracy\": accuracy, \"F1-Score\": f1, \"ROC-AUC\": roc_auc}\n",
        "    return results\n",
        "\n",
        "# Define the models\n",
        "feature_based_models = {\n",
        "    \"k-NN\": k_NN,\n",
        "    \"Random Forest\": random_forest,\n",
        "    \"SVM\": svm,\n",
        "    \"Logistic Regression\": logistic_regression,\n",
        "    \"Naive Bayes\": naive_bayes,\n",
        "    \"MLP\": mlp,\n",
        "    \"Decision Tree\": decision_tree\n",
        "}\n",
        "sequence_based_models = {\n",
        "    \"RNN\": model_RNN,\n",
        "    \"LSTM\": model_LSTM,\n",
        "    \"GRU\": model_GRU,\n",
        "}\n",
        "\n",
        "#Find the results\n",
        "feature_results = evaluate_feature_models(\n",
        "    models=feature_based_models,\n",
        "    X_test_features=X_test_scaled,\n",
        "    y_test_features=y_test_features\n",
        ")\n",
        "sequence_results = evaluate_sequence_models(\n",
        "    models=sequence_based_models,\n",
        "    X_test_sequence=X_test_dense,\n",
        "    y_test_sequence=y_test_sequence\n",
        ")\n",
        "cnn_results = evaluate_cnn_model(\n",
        "    model=model_CNN,\n",
        "    X_test_cnn=X_test_dense,\n",
        "    y_test_cnn=y_test_sequence\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Combine results into a single dictionary\n",
        "all_results = {**feature_results, **sequence_results, \"CNN\": cnn_results}\n",
        "all_results_df = pd.DataFrame.from_dict(all_results, orient='index')\n",
        "\n",
        "# Print the results\n",
        "print(all_results_df)\n",
        "\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(all_results_df.index, all_results_df[\"Accuracy\"], color=\"skyblue\")\n",
        "plt.title(\"Model Accuracy Comparison\", fontsize=16)\n",
        "plt.ylabel(\"Accuracy\", fontsize=14)\n",
        "plt.xlabel(\"Model\", fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# Plot F1-Score\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(all_results_df.index, all_results_df[\"F1-Score\"], color=\"orange\")\n",
        "plt.title(\"Model F1-Score Comparison\", fontsize=16)\n",
        "plt.ylabel(\"F1-Score\", fontsize=14)\n",
        "plt.xlabel(\"Model\", fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC-AUC\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(all_results_df.index, all_results_df[\"ROC-AUC\"], color=\"green\")\n",
        "plt.title(\"Model ROC-AUC Comparison\", fontsize=16)\n",
        "plt.ylabel(\"ROC-AUC\", fontsize=14)\n",
        "plt.xlabel(\"Model\", fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# Combined Line Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(all_results_df.index, all_results_df[\"Accuracy\"], marker=\"o\", label=\"Accuracy\", color=\"skyblue\")\n",
        "plt.plot(all_results_df.index, all_results_df[\"F1-Score\"], marker=\"s\", label=\"F1-Score\", color=\"orange\")\n",
        "plt.plot(all_results_df.index, all_results_df[\"ROC-AUC\"], marker=\"^\", label=\"ROC-AUC\", color=\"green\")\n",
        "plt.title(\"Model Metrics Comparison\", fontsize=16)\n",
        "plt.ylabel(\"Metric Value\", fontsize=14)\n",
        "plt.xlabel(\"Model\", fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhjrRo7pAsSy"
      },
      "source": [
        "# Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpF5d01RAv3w"
      },
      "source": [
        "In this block I tune the hyperparameters for the feature based models to see if that will improve performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIrVsh8BA3Qj"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# Hyperparameter grids\n",
        "param_grids = {\n",
        "    \"k-NN\": {\n",
        "        \"n_neighbors\": [3, 5, 7, 9, 11, 13, 15, 17],\n",
        "        \"weights\": [\"uniform\", \"distance\"],\n",
        "        \"metric\": [\"euclidean\", \"manhattan\"]\n",
        "    },\n",
        "    \"Random Forest\": {\n",
        "        \"n_estimators\": [50, 100, 150, 200],\n",
        "        \"max_depth\": [None, 10, 15, 20, 25, 30],\n",
        "        \"min_samples_split\": [2, 5, 10]\n",
        "    },\n",
        "    \"MLP\": {\n",
        "        \"hidden_layer_sizes\": [(64,), (96,), (128,), (64, 32), (96, 32), (128, 64)],\n",
        "        \"activation\": [\"relu\", \"tanh\"],\n",
        "        \"solver\": [\"adam\", \"sgd\"],\n",
        "        \"alpha\": [0.0001, 0.001, 0.01, 0.1]\n",
        "    },\n",
        "}\n",
        "\n",
        "# Iterate through feature-based models\n",
        "best_models = {}\n",
        "tuning_results = {}\n",
        "for name, model in feature_based_models.items():\n",
        "    print(f\"Tuning hyperparameters for {name}...\")\n",
        "    param_grid = param_grids.get(name, {})\n",
        "    start_time = time.time()\n",
        "    # GridSearch\n",
        "    grid_search = GridSearchCV(model, param_grid, scoring=\"roc_auc\", cv=5, verbose=1, n_jobs=-1)\n",
        "    grid_search.fit(X_train_features, y_train_features)\n",
        "    best_models[name] = grid_search.best_estimator_\n",
        "    tuning_results[name] = {\n",
        "        \"Best Parameters\": grid_search.best_params_,\n",
        "        \"Best ROC-AUC\": grid_search.best_score_\n",
        "    }\n",
        "    end_time = time.time()\n",
        "    tuning_time = end_time - start_time\n",
        "    print(f\"Tuning Time: {tuning_time:.2f} seconds\\n\")\n",
        "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best ROC-AUC: {grid_search.best_score_}\\n\")\n",
        "\n",
        "# Evaluate the best models\n",
        "final_results = {}\n",
        "\n",
        "for name, model in best_models.items():\n",
        "    print(f\"Evaluating tuned {name}...\")\n",
        "\n",
        "    # Predict probabilities\n",
        "    y_pred_probs = model.predict_proba(X_test_features)[:, 1] if hasattr(model, \"predict_proba\") else model.predict(X_test_features)\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test_features, y_pred)\n",
        "    f1 = f1_score(y_test_features, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test_features, y_pred_probs)\n",
        "    final_results[name] = {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"F1-Score\": f1,\n",
        "        \"ROC-AUC\": roc_auc\n",
        "    }\n",
        "    print(f\"Tuned {name} - Accuracy: {accuracy:.3f}, F1-Score: {f1:.3f}, ROC-AUC: {roc_auc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twwPEexcI2Pe"
      },
      "source": [
        "In this block, I will try to tune the hyperparameters of the sequence based models. It can be seen that all three models originally bottlenecked at a certain level and would not perform better, so I will only tune one of them, the GRU, which seemed to perform better than the RNN and around the same level as the LSTM, but slightly faster"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_tuner"
      ],
      "metadata": {
        "id": "rl9OqkvtipbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vdwdst9JcSy"
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, GRU, SimpleRNN, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Function to define the model\n",
        "def build_GRU(hpm, model_type=\"GRU\"):\n",
        "    model = Sequential()\n",
        "    model.add(\n",
        "        Dense(\n",
        "            hpm.Int(\"units\", min_value=32, max_value=128, step=32),\n",
        "            activation=\"relu\",\n",
        "            input_shape=(X_train_dense.shape[1], X_train_dense.shape[2]),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    model.add(GRU(hpm.Int(\"recurrent_units\", min_value=32, max_value=128, step=32)))\n",
        "    model.add(Dropout(hpm.Float(\"dropout\", min_value=0.1, max_value=0.5, step=0.1)))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=Adam(hpm.Choice(\"learning_rate\", values=[0.001, 0.01, 0.1])),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=['accuracy'],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Hyperparameter Tuning\n",
        "tuner = kt.RandomSearch(\n",
        "    lambda hpm: build_GRU(hpm, model_type=\"GRU\"),\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory=\"tuning_results\",\n",
        "    project_name=\"gru_hyperband_tuning\",\n",
        ")\n",
        "X_train_dense\n",
        "# Run the tuner\n",
        "tuner.search(X_train_dense, y_train_sequence, epochs=10, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Get the best model\n",
        "best_GRU_hpms = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_GRU = tuner.get_best_models(num_models=1)[0]\n",
        "print(\"Best hyperparameters:\", best_GRU_hpms.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH-ypfbsK1-N"
      },
      "source": [
        "In this block, I will try to tune the hyperparameters of the CNN to see if there are any performance improvements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTDmraRfKzrO"
      },
      "outputs": [],
      "source": [
        "def build_cnn(hpm):\n",
        "    model = Sequential()\n",
        "    kernel_size = eval(hpm.Choice(\"kernel_size\", values=[\"(3, 3)\", \"(5, 5)\"]))\n",
        "    model.add(\n",
        "        Conv2D(\n",
        "            filters=hpm.Int(\"filters\", min_value=16, max_value=64, step=16),\n",
        "            kernel_size=kernel_size,\n",
        "            activation=\"relu\",\n",
        "            input_shape=(X_train_cnn.shape[1], X_train_cnn.shape[2], X_train_cnn.shape[3]),\n",
        "        )\n",
        "    )\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(hpm.Float(\"dropout\", min_value=0.1, max_value=0.5, step=0.1)))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(Dense(hpm.Int(\"dense_units\", min_value=64, max_value=256, step=64), activation=\"relu\"))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "    model.compile(\n",
        "        optimizer=Adam(hpm.Choice(\"learning_rate\", values=[0.001, 0.01, 0.1])),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Hyperparameter Tuning\n",
        "cnn_tuner = kt.RandomSearch(\n",
        "    build_cnn,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory=\"tuning_results\",\n",
        "    project_name=\"cnn_model_tuning\",\n",
        ")\n",
        "\n",
        "# Run the tuner\n",
        "cnn_tuner.search(X_train_dense, y_train_sequence, epochs=10, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Get the best CNN model\n",
        "best_cnn_hps = cnn_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_cnn = cnn_tuner.get_best_models(num_models=1)[0]\n",
        "print(\"Best hyperparameters for CNN:\", best_cnn_hps.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqQEkudDMddd"
      },
      "source": [
        "# Final Evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vin9BLCzMi1A"
      },
      "source": [
        "After the initial training, it was seen that the sequence-based models fell far behind both the feature-based models and the CNN. The Random Forest model outcompeted every other feature-based model to varying degrees, besides maybe the MLP, and provided the same level of discerment as the CNN, while being extremely fast to train in comparison to both the MLP and especially the CNN. After the hyperparameter tuning, these are the new results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OKvp-yiO85E"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Find the best feature-based model based on ROC-AUC\n",
        "feature_results_df = pd.DataFrame.from_dict(final_results, orient=\"index\")\n",
        "best_model_name = feature_results_df[\"ROC-AUC\"].idxmax()\n",
        "tuned_feature_model = best_models[feature_results_df[\"ROC-AUC\"].idxmax()]\n",
        "\n",
        "# Evaluate GRU\n",
        "gru_results = {}\n",
        "y_pred_probs_gru = best_GRU.predict(X_test_dense)\n",
        "y_pred_gru = (y_pred_probs_gru > 0.5).astype(int)\n",
        "gru_results[\"Accuracy\"] = accuracy_score(y_test_sequence, y_pred_gru)\n",
        "gru_results[\"F1-Score\"] = f1_score(y_test_sequence, y_pred_gru)\n",
        "gru_results[\"ROC-AUC\"] = roc_auc_score(y_test_sequence, y_pred_probs_gru)\n",
        "\n",
        "# Evaluate CNN\n",
        "cnn_results = {}\n",
        "y_pred_probs_cnn = best_cnn.predict(X_test_cnn)\n",
        "y_pred_cnn = (y_pred_probs_cnn > 0.5).astype(int)\n",
        "cnn_results[\"Accuracy\"] = accuracy_score(y_test_sequence, y_pred_cnn)\n",
        "cnn_results[\"F1-Score\"] = f1_score(y_test_sequence, y_pred_cnn)\n",
        "cnn_results[\"ROC-AUC\"] = roc_auc_score(y_test_sequence, y_pred_probs_cnn)\n",
        "\n",
        "print(\"GRU Results:\", gru_results)\n",
        "print(\"CNN Results:\", cnn_results)\n",
        "\n",
        "# Combine results\n",
        "comparison_results = pd.DataFrame({\n",
        "    \"Model\": [best_model_name, \"GRU\", \"CNN\"],\n",
        "    \"Accuracy\": [\n",
        "        feature_results_df.loc[best_model_name, \"Accuracy\"],\n",
        "        gru_results[\"Accuracy\"],\n",
        "        cnn_results[\"Accuracy\"]\n",
        "    ],\n",
        "    \"F1-Score\": [\n",
        "        feature_results_df.loc[best_model_name, \"F1-Score\"],\n",
        "        gru_results[\"F1-Score\"],\n",
        "        cnn_results[\"F1-Score\"]\n",
        "    ],\n",
        "    \"ROC-AUC\": [\n",
        "        feature_results_df.loc[best_model_name, \"ROC-AUC\"],\n",
        "        gru_results[\"ROC-AUC\"],\n",
        "        cnn_results[\"ROC-AUC\"]\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(comparison_results)\n",
        "\n",
        "# Plot the metrics and models\n",
        "metrics = [\"Accuracy\", \"F1-Score\", \"ROC-AUC\"]\n",
        "\n",
        "for metric in metrics:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(comparison_results[\"Model\"], comparison_results[metric], color=[\"skyblue\", \"orange\", \"green\"])\n",
        "    plt.title(f\"Model Comparison: {metric}\", fontsize=16)\n",
        "    plt.ylabel(metric, fontsize=14)\n",
        "    plt.xlabel(\"Model\", fontsize=14)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "for metric in metrics:\n",
        "    plt.plot(comparison_results[\"Model\"], comparison_results[metric], marker=\"o\", label=metric)\n",
        "\n",
        "plt.title(\"Model Performance Comparison\", fontsize=16)\n",
        "plt.ylabel(\"Metric Value\", fontsize=14)\n",
        "plt.xlabel(\"Model\", fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEnOGAlFGt9qEIVF65KJDO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}